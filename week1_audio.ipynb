{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c9acb8",
   "metadata": {},
   "source": [
    "# Intro to Digital Audio\n",
    "\n",
    "To be able to work with audio signals in a computer, we need to create a series of discrete values from a continuous sound wave.\n",
    "\n",
    "This notebook will walk you through the fundamentals of how audio is represented in Python, how to create a custom dataset of music using yt-dlp/musicdl, and how to visualize the waveform of an audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92e900",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/MichiganDataScienceTeam/F25-Shazam/blob/main/notebooks/week1_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ffeaa",
   "metadata": {},
   "source": [
    "# 1/3: Some Terminology\n",
    "\n",
    "\n",
    "Sounds are produced via vibrations of air particles which form sound waves. Sounds waves have the same 3 defining characteristics as other waves: frequency (which is the measure of the number of vibrations per second measured in Hz), amplitude (measure of the intensity of the sound which is taken in terms of dB), and phase (the state that the wave cycle that the sound is in at a single point in time).\n",
    "\n",
    "\n",
    "Sounds we hear, as shown in the waveform below, are produced by the combinations of waves of many different frequencies and amplitudes compounded together. Constructive and destructive interference influence how these waves interact with each other, increasing and decreasing the amplitude of each frequency accordingly. Understanding all of this process is not necessary but it is important to know that sound is a combination of many waves with varying loudness and pitch which come together in a way that humans can listen to and interpret.\n",
    "\n",
    "\n",
    "<img src=asset/wave_comp.png width=\"500\">\n",
    "\n",
    "\n",
    "Speakers make sounds by pushing a cone up and down according to the data in an audio file. This produces vibrations in the air to produce the sound waves that make the same sound as what was recorded from the original sound.\n",
    "\n",
    "\n",
    "For this reason, the main audio file formats of .mp3 and .wav files store an amplitude value amplitude at each time step. The \"amplitude\", usually measured in dB, again informs the position that the cone should be pushed to at any time with high values measuring moving the cones forward and low values meaning pulling the cone back in order to create the desired sound.\n",
    "\n",
    "\n",
    "<img src=asset/file.png width=\"500\">\n",
    "\n",
    "\n",
    "Now that we know how audio is stored, how are we able to go from a sound to an audio file in the first place? Well this is informed by two measures:\n",
    "\n",
    "\n",
    "- **sample rate** - number of samples per second (Hz). The sampling rate determines the time resolution of our representation The reciprocal of sampling rate is sampling interval which represents the time between samples. ex: If the sampling rate is 44100 times per second, then the sampling interval is 1/44100 sec.\n",
    "- **bit depth** - The number of bits used to store each sample. The larger the bit depth, the more precision we have to store frequency information (frequency resolution).\n",
    "\n",
    "\n",
    "The higher the sample rate and bit depth, the higher the resolution of audio we are able to play and record. A higher sample rate leads to more frequency measurements taken and a higher bit depth means that we have more possible frequencies (you can think of frequencies as notes for simplification) that the audio can take on when stored.\n",
    "\n",
    "\n",
    "For the sake of this project we will be using wav files instead of mp3 files due to wav files being uncompressed. Compressed audio files (like mp3s) do not retain all sonic information from an original recording in order to save memory overhead making them good for streaming. However, this is not ideal for audio processing and recognition due to the normalization of the volume and reduction of very high and very low frequency sounds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507dfb7",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Let's play around with the frequency and sample rate to see what information we are able to gather from the generated wave.\n",
    "\n",
    "The wave will be of the specified frequency and the dots are the samples taken from this wave for the given sample rate. \n",
    "\n",
    "Do you notice any relationship between sampling rate and frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470ce5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of the audio signal\n",
    "freq_Hz = 6\n",
    "\n",
    "# rate at which audio is being sampled\n",
    "sample_rate_Hz = 12\n",
    "\n",
    "x_continuous = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_continuous = np.cos(freq_Hz * 2*np.pi*x_continuous)\n",
    "\n",
    "x_discrete = np.linspace(0, 1, sample_rate_Hz, endpoint=False)\n",
    "y_discrete = np.cos(freq_Hz * 2*np.pi*x_discrete)\n",
    "\n",
    "plt.plot(x_continuous, y_continuous)\n",
    "plt.scatter(x_discrete, y_discrete, color=\"red\", s=50)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3c2bb",
   "metadata": {},
   "source": [
    "## Good to Know Sampling Theorem:\n",
    "\n",
    "\n",
    "If a signal contains no frequencies higher than $f_\\mathrm{max}$, then the signal can be perfectly reconstructed when sampled at a rate $sr > 2f_\\mathrm{max}$. In other words, the maximum reconstructable frequency is strictly less than $sr/2$. This is called the Nyquist-Shannon sampling theorem.\n",
    "\n",
    "\n",
    "It is because of this theorem that you probably noticed that sampling rates twice that of the frequency are able to sample exactly at each peak and valley. This proves that in order to fully capture information about a wave of a given frequency and thus be able to reconstruct it during audio playback, we need to sample at twice that frequency.\n",
    "\n",
    "\n",
    "If we do not have a high enough sampling rate we can run into the phenomenon of aliasing. When the sampling rate is not fast enough to accurately reconstruct the highest frequency waves in the waveform, which results in misrepresenting them as low frequencies. This is a big issue for shazam's algorithm since we need to be able to know the distribution of the frequencies of the original audio to make an accurate prediction.\n",
    "\n",
    "\n",
    "Question: In order to cover the human hearing range of 20 Hz to 20 kHz, what is the minimum sampling rate required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_rate_required = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set a sample rate to be able to reconstruct the original frequency\n",
    "sample_rate_Hz = 2\n",
    "\n",
    "freq_Hz = 5\n",
    "\n",
    "freq_reconstructable_up_to = sample_rate_Hz / 2\n",
    "\n",
    "# this is the original continuous wave\n",
    "x_continuous = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_continuous = np.cos(freq_Hz * 2*np.pi*x_continuous)\n",
    "\n",
    "# this is the highest frequency we can reconstruct without aliasing\n",
    "x_undersampled = np.linspace(0, 1, 1000, endpoint=False)\n",
    "y_undersampled = np.cos(freq_reconstructable_up_to * 2*np.pi*x_undersampled)\n",
    "\n",
    "x_discrete = np.linspace(0, 1, sample_rate_Hz, endpoint=False)\n",
    "y_discrete = np.cos(freq_Hz * 2*np.pi*x_discrete)\n",
    "\n",
    "plt.plot(x_continuous, y_continuous)\n",
    "plt.plot(x_undersampled, y_undersampled, color=\"red\", linestyle=\"--\")\n",
    "plt.scatter(x_discrete, y_discrete, color=\"red\", s=50)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61deb37b",
   "metadata": {},
   "source": [
    "# 2/3: Obtaining audio files\n",
    "\n",
    "To create a database of songs that our Shazam clone will be able to recognize, we can use the [yt-dlp](https://github.com/yt-dlp/yt-dlp) Python package to download audio files directly from YouTube.\n",
    "\n",
    "Find a music video from youtube and paste the url below to see if you can download and play it's audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97125266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import os\n",
    "\n",
    "# TODO: add any youtube video url here, copied from browser address bar\n",
    "youtube_url = \"\"\n",
    "\n",
    "# name of the output audio file the video will be stored in\n",
    "yt_audio_path = \"yt_sample.wav\"\n",
    "\n",
    "# downloading options\n",
    "ydl_opts = {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': \"yt_sample.%(ext)s\",  # output file\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'wav',     # save as wav file\n",
    "        }],\n",
    "        #'cookiefile': 'cookies.txt',\n",
    "    }\n",
    "\n",
    "# download only if youtube_url is not empty\n",
    "if youtube_url != \"\":\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([youtube_url])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d34d356",
   "metadata": {},
   "source": [
    "## musicdl: helper script for downloading audio\n",
    "\n",
    "For convienience, we've created a yt-dlp wrapper program that accepts urls from either YouTube or Spotify, and downloads the respective audio files to a specified folder.\n",
    "\n",
    "Install the program below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/dennisfarmer/musicdl.git\n",
    "%pip install -e ./musicdl\n",
    "from musicdl.yt import YoutubeDownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b074ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run help to see information about the download function\n",
    "help(YoutubeDownloader.download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e1d95",
   "metadata": {},
   "source": [
    "# Excercise\n",
    "\n",
    "Use musicdl to download the audio of a YouTube video again like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "ydl = YoutubeDownloader(\n",
    "    audio_directory=\"./tracks\", \n",
    "    audio_format=\"wav\"\n",
    ")\n",
    "\n",
    "# TODO: add any addition youtube video urls here that you want to download\n",
    "youtube_urls = [\n",
    "    \"https://www.youtube.com/watch?v=TqxfdNm4gZQ\"\n",
    "]\n",
    "\n",
    "# will download the audio files to ./tracks\n",
    "tracks_info = ydl.download(youtube_urls)\n",
    "\n",
    "# print information stores about the downloaded tracks\n",
    "pprint(tracks_info)\n",
    "\n",
    "# write to csv\n",
    "tracks_csv = \"./tracks/tracks_info.csv\"\n",
    "tracks_df = pd.DataFrame(tracks_info)\n",
    "tracks_df.to_csv(tracks_csv, index=False)\n",
    "\n",
    "# read from csv\n",
    "#with open(tracks_csv, \"r\") as f:\n",
    "    #tracks_df = pd.read_csv(f)\n",
    "    #tracks_list = list(tracks_df.to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0884c",
   "metadata": {},
   "source": [
    "# 3/3: Working with audio data in Python:\n",
    "\n",
    "## Visualizing the amplitude and sample rate of an audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac27996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_path = yt_audio_path\n",
    "audio_path = \"sample.wav\"\n",
    "\n",
    "# load the audio file into a numpy array with librosa\n",
    "# default is to convert to mono (1 channel)\n",
    "audio, sr = librosa.load(audio_path, sr=None)  # sr=None uses the file's sampling rate\n",
    "print(f\"sample rate = {sr} Hz\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(audio, lw=0.1)  # interpolated waveform\n",
    "plt.scatter(np.arange(len(audio)), audio, s=0.005, color=\"red\")  # individual samples\n",
    "plt.title(\"Audio Waveform\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "print(\"audio is digitally represented as a numpy.ndarray:\")\n",
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df28416",
   "metadata": {},
   "source": [
    "As you can see from above, files are downloaded from ytp_dl with an ideal sampling rate of 48 kHZ (1 kHZ = 1,000 HZ) and can be converted into a nump array using librosa. Values in the librosa audio array represent the value amplitude at each time index which can be visualized in the waveform above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e8a67",
   "metadata": {},
   "source": [
    "## Downsampling\n",
    "\n",
    "https://en.wikipedia.org/wiki/Downsampling_(signal_processing)\n",
    "\n",
    "Downsampling is the process of reducing the sampling rate of a file. Downsampling is important for shazam's algorithm because it allows us to standardize the sampling rate of our files while also compressing our audio to make the fingerprinting process faster.\n",
    "\n",
    "internally, downsampling (reducing the sample rate) performs the following process:\n",
    "1. uses a low pass filter to remove higher frequency components to avoid aliasing (Sampling Theorem)\n",
    "    - low pass: keep signals with frequency lower than a specified cutoff frequency\n",
    "    - question: what would the cutoff frequency be if we resampled to 11,025 Hz?\n",
    "2. keeps every nth sample using a sample step size (\"decimation factor\") of $(\\mathrm{sr_{old}}/\\mathrm{sr_{new}})$, \n",
    "    - the details of this process are not super important to know\n",
    "    - if $(\\mathrm{sr_{old}}/\\mathrm{sr_{new}})$ is not an integer, first upsamples via interpolation, then downsamples with step size $\\mathrm{sr_{old}}$.\n",
    "    - upsampling: each original sample is separated by inserting $(\\mathrm{sr_{new}}-1)$ zeros, then signal is interpolated with filter to smooth out discontinuities (replacing zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample from 48,000 Hz to 11,025 Hz\n",
    "sr = 11_025\n",
    "audio, sr = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "# cutoff_frequency = ???\n",
    "\n",
    "# Question: how would you plot just the first 10 seconds of an audio file?\n",
    "#            Modify the existing code to do this\n",
    "# Hint: use sample rate\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(audio, lw=0.1)  # interpolated waveform\n",
    "plt.scatter(np.arange(len(audio)), audio, s=0.005, color=\"red\")  # individual samples\n",
    "plt.title(\"Resampled Audio Waveform\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653caec9",
   "metadata": {},
   "source": [
    "# Task for today:\n",
    "\n",
    "Using our code examples above and resources from online (PLEADE DONT USE GEN AI (ChatGPT, Claude, etc)! If you have questions ask us or you will probably not learn anything from this project) complete the following tasks.\n",
    "\n",
    "1) download the audio from any youtube video as a .wav file\n",
    "2) downsample the file to 44.1 kHz\n",
    "3) crop the audio to the first 10 seconds\n",
    "4) visualize the waveform\n",
    "5) (optional) do the same thing with some recorded audio with pyaudio (try to figure this out yourself with pyaudio documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963ff41",
   "metadata": {},
   "source": [
    "---\n",
    "# Next week: visualizing frequency using a spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "audio_path = \"log_scale_perception.wav\"\n",
    "ipd.Audio(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee19bbe",
   "metadata": {},
   "source": [
    "## Method 1: Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b29fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "\n",
    "# parameters of the short-time Fourier transform:\n",
    "# (algorithm that creates the spectrogram)\n",
    "win_length = 2**11  # number of samples in each window\n",
    "n_fft = win_length\n",
    "hop_length = win_length // 4\n",
    "window = scipy.signal.get_window(\"triang\", Nx=win_length)\n",
    "\n",
    "S = librosa.stft(audio, \n",
    "                       n_fft=n_fft, hop_length=hop_length, \n",
    "                       win_length=win_length, window=window)\n",
    "S_magnitude = np.abs(S)  # |a+bi| = sqrt(a^2 + b^2)\n",
    "S_db = librosa.amplitude_to_db(S_magnitude, ref=np.max)\n",
    "\n",
    "im = plt.imshow(S_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bfa7c4",
   "metadata": {},
   "source": [
    "## Method 2: Scipy (what we'll use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07492ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "\n",
    "# parameters of the short-time Fourier transform:\n",
    "# (algorithm that creates the spectrogram)\n",
    "nperseg = win_length = 2**11  # number of samples in each window\n",
    "nfft = n_fft = win_length\n",
    "hop_length = win_length // 4\n",
    "window = scipy.signal.get_window(\"triang\", Nx=win_length)\n",
    "\n",
    "# scipy.signal.stft also uses the sample rate to output \n",
    "# frequency (in Hz) and time (in seconds) vectors,\n",
    "# corresponding to the rows and columns of the stft matrix \n",
    "# in \"s_scipy\"\n",
    "fs=sr \n",
    "noverlap = nperseg - hop_length\n",
    "\n",
    "freq_scipy, time_scipy, s_scipy = scipy.signal.stft(\n",
    "    audio, \n",
    "    fs=fs, window=\"hann\", nfft=nfft, \n",
    "    nperseg=nperseg, noverlap=noverlap\n",
    ")\n",
    "\n",
    "print(f\"freq vector shape: {freq_scipy.shape}\")\n",
    "print(f\"time vector shape: {time_scipy.shape}\")\n",
    "print(f\"stft matrix shape: {s_scipy.shape}\")\n",
    "\n",
    "s_scipy_db = librosa.amplitude_to_db(np.abs(s_scipy), ref=np.max)\n",
    "\n",
    "im = plt.imshow(s_scipy_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7dac42",
   "metadata": {},
   "source": [
    "## Recording from Microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import pyaudio\n",
    "\n",
    "def record_audio(n_seconds: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    records audio using computer microphone\n",
    "    \"\"\"\n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 48000\n",
    "\n",
    "\n",
    "    input(\"Press Enter to begin recording 🎤\")\n",
    "    print(\"🎤 Listening for music\", end=\"\\r\")\n",
    "\n",
    "    outfile = \"microphone_sample.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * n_seconds)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "\n",
    "    audio_np = np.frombuffer(b''.join(frames), dtype=np.int16)\n",
    "    scipy.io.wavfile.write(outfile, RATE, audio_np)\n",
    "    print(f\"✅ Recording saved to {outfile}\", end=\"\\n\")\n",
    "\n",
    "    return outfile\n",
    "\n",
    "audio_path = record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799cf718",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"microphone_sample.wav\"\n",
    "\n",
    "audio, sr = librosa.load(audio_path, sr=None) \n",
    "S_db = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "im = plt.imshow(S_db, cmap=\"inferno\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.colorbar(im, format=\"%+2.0f dB\")\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
